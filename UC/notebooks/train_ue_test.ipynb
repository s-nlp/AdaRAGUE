{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from lm_polygraph.estimators import *\n",
    "from lm_polygraph.utils.model import WhiteboxModel\n",
    "from lm_polygraph.utils.dataset import Dataset\n",
    "from lm_polygraph.utils.processor import Logger\n",
    "from lm_polygraph.utils.manager import UEManager\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"bigscience/bloomz-560m\"\n",
    "device = \"cuda\"\n",
    "dataset_name = (\"trivia_qa\", \"rc.nocontext\")\n",
    "batch_size = 4\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = WhiteboxModel(base_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7708d6b1befe4cb59ee37ae97cfbe67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7368166366ef4d85946d4bf7dffbd73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use validation split, since test split of trivia_qa doesn't have reference answers\n",
    "dataset = Dataset.load(\n",
    "    dataset_name,\n",
    "    'question', 'answer',\n",
    "    batch_size=batch_size,\n",
    "    prompt=\"Question: {question}\\nAnswer:{answer}\",\n",
    "    split=\"validation\"\n",
    ")\n",
    "dataset.subsample(16, seed=seed)\n",
    "\n",
    "train_dataset = Dataset.load(\n",
    "    dataset_name,\n",
    "    'question', 'answer',\n",
    "    batch_size=batch_size,\n",
    "    prompt=\"Question: {question}\\nAnswer:{answer}\",\n",
    "    split=\"train\"\n",
    ")\n",
    "train_dataset.subsample(16, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_methods = [MahalanobisDistanceSeq(\"decoder\"),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat calculators: [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator object at 0x7f205f1f9600>, <lm_polygraph.stat_calculators.embeddings.EmbeddingsCalculator object at 0x7f205f1fb9a0>]\n"
     ]
    }
   ],
   "source": [
    "man = UEManager(\n",
    "    dataset,\n",
    "    model,\n",
    "    ue_methods,\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    train_data=train_dataset,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [<lm_polygraph.stat_calculators.embeddings.EmbeddingsCalculator at 0x7fb469f87130>],\n",
       " [<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator at 0x7fb469f87b20>,\n",
       "  <lm_polygraph.stat_calculators.embeddings.EmbeddingsCalculator at 0x7fb469f87130>])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man.background_train_stat_calculators, man.train_stat_calculators, man.stat_calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<lm_polygraph.stat_calculators.embeddings.EmbeddingsCalculator at 0x7fb469f87130>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am estimator and I have the following stat keys dict_keys(['input_texts', 'target_texts', 'model', 'train_embeddings_decoder', 'input_tokens', 'greedy_log_probs', 'greedy_tokens', 'greedy_tokens_alternatives', 'greedy_texts', 'greedy_log_likelihoods', 'embeddings_decoder'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:03<00:03,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am estimator and I have the following stat keys dict_keys(['input_texts', 'target_texts', 'model', 'input_tokens', 'greedy_log_probs', 'greedy_tokens', 'greedy_tokens_alternatives', 'greedy_texts', 'greedy_log_likelihoods', 'embeddings_decoder'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am estimator and I have the following stat keys dict_keys(['input_texts', 'target_texts', 'model', 'input_tokens', 'greedy_log_probs', 'greedy_tokens', 'greedy_tokens_alternatives', 'greedy_texts', 'greedy_log_likelihoods', 'embeddings_decoder'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am estimator and I have the following stat keys dict_keys(['input_texts', 'target_texts', 'model', 'input_tokens', 'greedy_log_probs', 'greedy_tokens', 'greedy_tokens_alternatives', 'greedy_texts', 'greedy_log_likelihoods', 'embeddings_decoder'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = man()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('sequence',\n",
       "              'MahalanobisDistanceSeq_decoder'): [1169.6920166015625, 826.4873046875, 1069.16357421875, 1156.1181640625, 708.9137573242188, 963.9560546875, 819.7874145507812, 1190.7142333984375, 1264.18798828125, 812.9637451171875, 943.4371948242188, 3326.961669921875, 716.6776123046875, 928.23583984375, 860.3135375976562, 1123.5225830078125]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man.estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_texts', 'target_texts', 'greedy_texts', 'greedy_tokens'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man.stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "train_stats = man._extract_train_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_embeddings_decoder': array([[ 8.0123323e-01,  3.3615193e-01, -9.2046976e-01, ...,\n",
       "         -8.7836536e+02, -3.0853969e-01, -4.1382957e+00],\n",
       "        [ 5.1549762e-01,  2.0519648e+00, -1.2289014e+00, ...,\n",
       "         -8.6393359e+02, -2.0847676e+00,  7.3566306e-01],\n",
       "        [-1.4037980e-01,  2.2656708e+00, -1.8202316e+00, ...,\n",
       "         -7.8453168e+02,  3.2707655e-01, -3.3860345e+00],\n",
       "        ...,\n",
       "        [ 3.4297333e+00,  1.3884777e-01, -2.1633844e+00, ...,\n",
       "         -5.3989429e+02, -7.8078997e-01, -4.1057215e+00],\n",
       "        [ 3.0026469e+00,  2.2823787e+00, -3.4084189e-01, ...,\n",
       "         -8.5602466e+02, -2.8812864e-01, -2.2269652e+00],\n",
       "        [ 2.4519567e+00,  3.6791754e+00, -2.6558270e+00, ...,\n",
       "         -6.4472070e+02,  1.7205124e+00, -6.2113414e+00]], dtype=float32)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<lm_polygraph.stat_calculators.greedy_probs.GreedyProbsCalculator at 0x7fb4ec705ae0>,\n",
       " <lm_polygraph.stat_calculators.embeddings.EmbeddingsCalculator at 0x7fb4ec705330>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man.stat_calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man.background_train_stat_calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<lm_polygraph.stat_calculators.embeddings.EmbeddingsCalculator at 0x7fb4ec705330>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man.train_stat_calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12510.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#stats = {}\n",
    "iterable_data = tqdm(man.data)\n",
    "for batch_i, (inp_texts, target_texts) in enumerate(iterable_data):\n",
    "    batch_stats = {}\n",
    "    for key, val in [\n",
    "        (\"input_texts\", inp_texts),\n",
    "        (\"target_texts\", target_texts),\n",
    "    ]:\n",
    "       # stats[key] += val\n",
    "        batch_stats[key] = val\n",
    "    batch_stats[\"model\"] = man.model\n",
    "\n",
    "    train_stats_keys = list(train_stats.keys())\n",
    "    for stat in train_stats_keys:\n",
    "        batch_stats[stat] = train_stats.pop(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_stats['input_texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_stat = train_stats.pop('train_embeddings_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_stat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
