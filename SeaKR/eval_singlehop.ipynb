{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Tuple, Union, Callable, Literal\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class Evaluator:\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_answer(cls, s):\n",
    "        def remove_articles(text):\n",
    "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "        def white_space_fix(text):\n",
    "            return ' '.join(text.split())\n",
    "        def remove_punc(text):\n",
    "            exclude = set(string.punctuation)\n",
    "            return ''.join(ch for ch in text if ch not in exclude)\n",
    "        def lower(text):\n",
    "            return text.lower()\n",
    "        if not isinstance(s, str):\n",
    "            return \"\"\n",
    "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "    @classmethod\n",
    "    def exact_match_score(\n",
    "        cls,\n",
    "        prediction: str,\n",
    "        ground_truth: Union[str, List[str]],\n",
    "    ):\n",
    "        if not prediction:\n",
    "            return {'correct': 0, 'incorrect': 1}\n",
    "        ground_truths = {ground_truth} if isinstance(ground_truth, str) else set(ground_truth)\n",
    "\n",
    "        correct = np.max([int(cls.normalize_answer(prediction) == cls.normalize_answer(gt)) for gt in ground_truths])\n",
    "        return {'correct': correct, 'incorrect': 1 - correct}\n",
    "\n",
    "    @classmethod\n",
    "    def f1_score(\n",
    "        cls,\n",
    "        prediction: str,\n",
    "        ground_truth: Union[str, List[str]],\n",
    "    ):\n",
    "        final_metric = {'f1': 0, 'precision': 0, 'recall': 0}\n",
    "        \n",
    "        if not prediction:\n",
    "            return final_metric\n",
    "        ground_truths = {ground_truth} if isinstance(ground_truth, str) else set(ground_truth)\n",
    "            \n",
    "        for ground_truth in ground_truths:\n",
    "            normalized_prediction = cls.normalize_answer(prediction)\n",
    "            normalized_ground_truth = cls.normalize_answer(ground_truth)\n",
    "            if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "                continue\n",
    "            if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "                continue\n",
    "            prediction_tokens = normalized_prediction.split()\n",
    "            ground_truth_tokens = normalized_ground_truth.split()\n",
    "            common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "            num_same = sum(common.values())\n",
    "            if num_same == 0:\n",
    "                continue\n",
    "\n",
    "            precision = 1.0 * num_same / len(prediction_tokens)\n",
    "            recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "            for k in ['f1', 'precision', 'recall']:\n",
    "                final_metric[k] = max(eval(k), final_metric[k])\n",
    "        return final_metric\n",
    "    \n",
    "    def eval_answer(self, results_df, answer_col=\"Final Answer\"):\n",
    "        # for datasets don't have answer_ids, aliases\n",
    "        em_list = []\n",
    "        f1_list = []\n",
    "        for i, row in results_df.iterrows():\n",
    "            prediction = row[answer_col]\n",
    "            ground_truth = row['ground_truth']\n",
    "            em_list.append(self.exact_match_score(prediction, ground_truth)['correct'])\n",
    "            f1_list.append(self.f1_score(prediction, ground_truth)['f1'])\n",
    "        print(f\"EM: {sum(em_list)/len(em_list):4f}\\t F1: {sum(f1_list)/len(f1_list):4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_eval(pred_list, ground_truths):\n",
    "    evaluator = Evaluator()\n",
    "    em_list = []\n",
    "    f1_list = []\n",
    "    for prediction, ground_truth in zip(pred_list, ground_truths):\n",
    "        em_list.append(evaluator.exact_match_score(prediction, ground_truth)['correct'])\n",
    "        f1_list.append(evaluator.f1_score(prediction, ground_truth)['f1'])\n",
    "    print(f\"EM: {sum(em_list)/len(em_list):4f}\\t F1: {sum(f1_list)/len(f1_list):4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name: Literal['nq', 'tq', 'sq'] = 'nq'\n",
    "\n",
    "raw_data = pd.read_json(f\"./data/singlehop_data/processed_{dataset_name}.json\")\n",
    "ground_truth_list = raw_data['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_results = pd.read_json(\"./outputs_nq/direct.jsonl\", lines=True) # replace with your output file\n",
    "rag_results = pd.read_json(\"./outputs_nq/rag.jsonl\", lines=True) # replace with your output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ground_truth_list) == len(direct_results)\n",
    "assert 10 * len(ground_truth_list) == len(rag_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.251524\t F1: 0.351987\n"
     ]
    }
   ],
   "source": [
    "## Llama 2\n",
    "THRESHOLD = -6.0\n",
    "answer_list = []\n",
    "for i, direct_res in direct_results.iterrows():\n",
    "    direct_eigen_score = direct_res['eigen_score']\n",
    "    if direct_eigen_score < THRESHOLD:\n",
    "        answer_list.append(direct_res['answer'])\n",
    "    else:\n",
    "        rag_batch = rag_results.iloc[10*i : 10*i+10]\n",
    "        best_answer = rag_batch.loc[rag_batch['eigen_score'].idxmin()]['answer']\n",
    "        answer_list.append(best_answer)\n",
    "my_eval(answer_list, ground_truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.337119\t F1: 0.443897\n"
     ]
    }
   ],
   "source": [
    "## Llama 3.1\n",
    "THRESHOLD = -6.0\n",
    "answer_list = []\n",
    "for i, direct_res in direct_results.iterrows():\n",
    "    direct_eigen_score = direct_res['eigen_score']\n",
    "    if direct_eigen_score < THRESHOLD:\n",
    "        answer_list.append(direct_res['answer'])\n",
    "    else:\n",
    "        rag_batch = rag_results.iloc[10*i : 10*i+10]\n",
    "        best_answer = rag_batch.loc[rag_batch['eigen_score'].idxmin()]['answer']\n",
    "        answer_list.append(best_answer)\n",
    "my_eval(answer_list, ground_truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NQ_Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name: Literal['nq', 'tq', 'sq'] = 'nq'\n",
    "\n",
    "raw_data = pd.read_json(f\"./data/singlehop_data/processed_{dataset_name}.json\") # replace with your processed dataset file\n",
    "ground_truth_list = raw_data['answer']\n",
    "\n",
    "direct_results = pd.read_json(\"./outputs_nq_ours_3_1/direct.jsonl\", lines=True) # replace with your output file\n",
    "rag_results = pd.read_json(\"./outputs_nq_ours_3_1/rag.jsonl\", lines=True) # replace with your output file\n",
    "\n",
    "data_ = pd.read_json('./data/singlehop_data/nq_top10.json')\n",
    "data_['len_ctxs'] = data_.ctxs.apply(lambda x: len(x))\n",
    "direct_results['len_ctxs']=data_.len_ctxs\n",
    "\n",
    "assert len(ground_truth_list) == len(direct_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559f0d7a9dfe483a857f2c073afb07f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.358000\t F1: 0.483508\n"
     ]
    }
   ],
   "source": [
    "## 3.1\n",
    "THRESHOLD = -6.0\n",
    "answer_list = []\n",
    "current_index = 0  # Pointer to keep track of the current position in rag_results\n",
    "\n",
    "for i, direct_res in tqdm(direct_results.iterrows()):\n",
    "    direct_eigen_score = direct_res['eigen_score']\n",
    "    \n",
    "    if direct_eigen_score < THRESHOLD:\n",
    "        answer_list.append(direct_res['answer'])\n",
    "        \n",
    "    else:\n",
    "        len_ctxs = direct_res['len_ctxs']\n",
    "        rag_batch = rag_results.iloc[current_index : current_index + len_ctxs]\n",
    "\n",
    "        best_rag_result = rag_batch.loc[rag_batch['eigen_score'].idxmin()]\n",
    "        best_answer = rag_batch.loc[rag_batch['eigen_score'].idxmin()]['answer']\n",
    "        \n",
    "        answer_list.append(best_answer)\n",
    "        current_index += len_ctxs\n",
    "        \n",
    "my_eval(answer_list, ground_truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nq_ours.jsonl', 'w', encoding='utf-8') as f:\n",
    "    pd.DataFrame([answer_list, ground_truth_list]).T.rename(columns={0:'predict', 1:'answer'}).to_json(f, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 7078.83it/s]\n",
      "Mean Accuracy: 0.4060\n",
      "Mean EM: 0.3600\n",
      "Mean F1: 0.4870\n"
     ]
    }
   ],
   "source": [
    "! python eval.py ./nq_ours.jsonl 'predict' 'answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: 100%|██████████████████████████| 1000/1000 [00:00<00:00, 2860.47it/s]\n",
      "--------------------------------------------------\n",
      " Accuracy (init, mean, median): 0.406, 0.406, 0.406 |Std: 0.023 | 95% CI: (0.36, 0.448)\n",
      " EM (init, mean, median): 0.36 , 0.36 , 0.36  | Std: 0.022 | 95% CI: (0.316, 0.404)\n",
      " F1 (init, mean, median): 0.487, 0.487, 0.488  | Std: 0.021 |95% CI: (0.445, 0.528)\n"
     ]
    }
   ],
   "source": [
    "! python bootrstrap_CI_upd.py --input_file \"nq_ours.jsonl\"\\\n",
    "                            --pred_col \"predict\"\\\n",
    "                            --gt_col \"answer\"\\\n",
    "                            --n_rounds 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQuad Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name: Literal['nq', 'tq', 'sq'] = 'sq'\n",
    "\n",
    "raw_data = pd.read_json(f\"./data/singlehop_data/processed_{dataset_name}.json\")\n",
    "ground_truth_list = raw_data['answer']\n",
    "\n",
    "direct_results = pd.read_json(f\"./outputs_{dataset_name}_ours_3_1/direct.jsonl\", lines=True) # replace with your output file\n",
    "rag_results = pd.read_json(f\"./outputs_{dataset_name}_ours_3_1/rag.jsonl\", lines=True) # replace with your output file\n",
    "\n",
    "data_ = pd.read_json('./data/singlehop_data/sq_top10.json')\n",
    "data_.len_ctxs = data_.len_ctxs.apply(lambda x: x[0]) \n",
    "direct_results['len_ctxs']=data_.len_ctxs\n",
    "\n",
    "\n",
    "assert len(ground_truth_list) == len(direct_results)\n",
    "# assert 12 * len(ground_truth_list) == len(rag_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd18fa6aedfd4a4d9b283d11e0d8b893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.228000\t F1: 0.353668\n"
     ]
    }
   ],
   "source": [
    "## 3.1\n",
    "THRESHOLD = -6.0\n",
    "answer_list = []\n",
    "current_index = 0  # Pointer to keep track of the current position in rag_results\n",
    "\n",
    "for i, direct_res in tqdm(direct_results.iterrows()):\n",
    "    direct_eigen_score = direct_res['eigen_score']\n",
    "    \n",
    "    if direct_eigen_score < THRESHOLD:\n",
    "        answer_list.append(direct_res['answer'])\n",
    "        \n",
    "    else:\n",
    "        len_ctxs = direct_res['len_ctxs']\n",
    "        rag_batch = rag_results.iloc[current_index : current_index + len_ctxs]\n",
    "\n",
    "        best_rag_result = rag_batch.loc[rag_batch['eigen_score'].idxmin()]\n",
    "        best_answer = rag_batch.loc[rag_batch['eigen_score'].idxmin()]['answer']\n",
    "        \n",
    "        answer_list.append(best_answer)\n",
    "        current_index += len_ctxs\n",
    "        \n",
    "my_eval(answer_list, ground_truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('squad_ours.jsonl', 'w', encoding='utf-8') as f:\n",
    "    pd.DataFrame([answer_list, ground_truth_list]).T.rename(columns={0:'predict', 1:'answer'}).to_json(f, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 6735.57it/s]\n",
      "Mean Accuracy: 0.2680\n",
      "Mean EM: 0.2260\n",
      "Mean F1: 0.3612\n"
     ]
    }
   ],
   "source": [
    "! python eval.py ./squad_ours.jsonl 'predict' 'answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: 100%|██████████████████████████| 1000/1000 [00:00<00:00, 2892.41it/s]\n",
      "--------------------------------------------------\n",
      " Accuracy (init, mean, median): 0.268, 0.268, 0.268 |Std: 0.02 | 95% CI: (0.23, 0.306)\n",
      " EM (init, mean, median): 0.226, 0.226, 0.226 | Std: 0.019 | 95% CI: (0.19, 0.262)\n",
      " F1 (init, mean, median): 0.361, 0.361, 0.361  | Std: 0.019 |95% CI: (0.323, 0.4)\n"
     ]
    }
   ],
   "source": [
    "! python bootrstrap_CI_upd.py --input_file \"squad_ours.jsonl\"\\\n",
    "                            --pred_col \"predict\"\\\n",
    "                            --gt_col \"answer\"\\\n",
    "                            --n_rounds 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TQ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name: Literal['nq', 'tq', 'sq'] = 'tq'\n",
    "\n",
    "raw_data = pd.read_json(f\"./data/singlehop_data/processed_{dataset_name}.json\")\n",
    "ground_truth_list = raw_data['answer']\n",
    "\n",
    "direct_results = pd.read_json(f\"./outputs_{dataset_name}_ours_3_1/direct.jsonl\", lines=True) # replace with your output file\n",
    "rag_results = pd.read_json(f\"./outputs_{dataset_name}_ours_3_1/rag.jsonl\", lines=True) # replace with your output file\n",
    "data_ = pd.read_json(f'./data/singlehop_data/{dataset_name}_top10.json')\n",
    "data_.len_ctxs = data_.len_ctxs.apply(lambda x: x[0]) \n",
    "direct_results['len_ctxs']=data_.len_ctxs\n",
    "\n",
    "assert len(ground_truth_list) == len(direct_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b306f9a4ed4eca8e5ce8137b770ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.598000\t F1: 0.692176\n"
     ]
    }
   ],
   "source": [
    "## 3.1\n",
    "THRESHOLD = -6.0\n",
    "answer_list = []\n",
    "current_index = 0  # Pointer to keep track of the current position in rag_results\n",
    "\n",
    "for i, direct_res in tqdm(direct_results.iterrows()):\n",
    "    direct_eigen_score = direct_res['eigen_score']\n",
    "    \n",
    "    if direct_eigen_score < THRESHOLD:\n",
    "        answer_list.append(direct_res['answer'])\n",
    "        \n",
    "    else:\n",
    "        len_ctxs = direct_res['len_ctxs']\n",
    "        rag_batch = rag_results.iloc[current_index : current_index + len_ctxs]\n",
    "\n",
    "        best_rag_result = rag_batch.loc[rag_batch['eigen_score'].idxmin()]\n",
    "        best_answer = rag_batch.loc[rag_batch['eigen_score'].idxmin()]['answer']\n",
    "        \n",
    "        answer_list.append(best_answer)\n",
    "        current_index += len_ctxs\n",
    "        \n",
    "my_eval(answer_list, ground_truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('tq_ours.jsonl', 'w', encoding='utf-8') as f:\n",
    "    pd.DataFrame([answer_list, ground_truth_list]).T.rename(columns={0:'predict', 1:'answer'}).to_json(f, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2183.41it/s]\n",
      "Mean Accuracy: 0.6560\n",
      "Mean EM: 0.5980\n",
      "Mean F1: 0.6955\n"
     ]
    }
   ],
   "source": [
    "! python eval.py ./tq_ours.jsonl 'predict' 'answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: 100%|██████████████████████████| 1000/1000 [00:00<00:00, 2902.51it/s]\n",
      "--------------------------------------------------\n",
      " Accuracy (init, mean, median): 0.656, 0.656, 0.656 |Std: 0.021 | 95% CI: (0.616, 0.698)\n",
      " EM (init, mean, median): 0.598, 0.597, 0.596 | Std: 0.022 | 95% CI: (0.556, 0.642)\n",
      " F1 (init, mean, median): 0.696, 0.695, 0.695  | Std: 0.019 |95% CI: (0.659, 0.732)\n"
     ]
    }
   ],
   "source": [
    "! python bootrstrap_CI_upd.py --input_file \"tq_ours.jsonl\"\\\n",
    "                            --pred_col \"predict\"\\\n",
    "                            --gt_col \"answer\"\\\n",
    "                            --n_rounds 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
